{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find polarity of unique products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install nltk packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>777</td>\n",
       "      <td>AV1YnR7wglJLPUi8IJmi</td>\n",
       "      <td>4</td>\n",
       "      <td>Great taffy at a great price.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>767</td>\n",
       "      <td>AVpfpK8KLJeJML43BCuD</td>\n",
       "      <td>4</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1080</td>\n",
       "      <td>AVqkIdntQMlgsOJE6fuB</td>\n",
       "      <td>5</td>\n",
       "      <td>Love this dress! it's sooo pretty.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProductID                UserID  Rating  \\\n",
       "0        777  AV1YnR7wglJLPUi8IJmi       4   \n",
       "1        767  AVpfpK8KLJeJML43BCuD       4   \n",
       "2       1080  AVqkIdntQMlgsOJE6fuB       5   \n",
       "\n",
       "                                                Text  \n",
       "0                      Great taffy at a great price.  \n",
       "1  Absolutely wonderful - silky and sexy and comf...  \n",
       "2                 Love this dress! it's sooo pretty.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cloths-rating.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 777,  767, 1080, 1077, 1049,  847,  858, 1095, 1065,  853, 1120,\n",
       "        697,  949, 1003,  684,  444, 1060, 1002,  862,  910,   89,  823,\n",
       "       6969, 9696,  333, 8001,  369], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_prod = df['ProductID'].unique()\n",
    "uni_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>777</td>\n",
       "      <td>AV1YnR7wglJLPUi8IJmi</td>\n",
       "      <td>4</td>\n",
       "      <td>Great taffy at a great price.</td>\n",
       "      <td>[Great, taffy, great, price, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>767</td>\n",
       "      <td>AVpfpK8KLJeJML43BCuD</td>\n",
       "      <td>4</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>[Absolutely, wonderful, -, silky, sexy, comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1080</td>\n",
       "      <td>AVqkIdntQMlgsOJE6fuB</td>\n",
       "      <td>5</td>\n",
       "      <td>Love this dress! it's sooo pretty.</td>\n",
       "      <td>[Love, dress, !, 's, sooo, pretty, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1077</td>\n",
       "      <td>AVpfpK8KLJeJML43BCuD</td>\n",
       "      <td>3</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>[I, high, hopes, dress, really, wanted, work, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1049</td>\n",
       "      <td>AVpfpK8KLJeJML43BCuD</td>\n",
       "      <td>5</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>[I, love, ,, love, ,, love, jumpsuit, ., 's, f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProductID                UserID  Rating  \\\n",
       "0        777  AV1YnR7wglJLPUi8IJmi       4   \n",
       "1        767  AVpfpK8KLJeJML43BCuD       4   \n",
       "2       1080  AVqkIdntQMlgsOJE6fuB       5   \n",
       "3       1077  AVpfpK8KLJeJML43BCuD       3   \n",
       "4       1049  AVpfpK8KLJeJML43BCuD       5   \n",
       "\n",
       "                                                Text  \\\n",
       "0                      Great taffy at a great price.   \n",
       "1  Absolutely wonderful - silky and sexy and comf...   \n",
       "2                 Love this dress! it's sooo pretty.   \n",
       "3  I had such high hopes for this dress and reall...   \n",
       "4  I love, love, love this jumpsuit. it's fun, fl...   \n",
       "\n",
       "                                          Clean_Text  \n",
       "0                    [Great, taffy, great, price, .]  \n",
       "1  [Absolutely, wonderful, -, silky, sexy, comfor...  \n",
       "2              [Love, dress, !, 's, sooo, pretty, .]  \n",
       "3   [I, high, hopes, dress, really, wanted, work, .]  \n",
       "4  [I, love, ,, love, ,, love, jumpsuit, ., 's, f...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clear_text(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in word_tokens if not w in stop]\n",
    "    try:\n",
    "        return filtered_text\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "df['Clean_Text'] = df['Text'].apply(clear_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_calc(clean_data):\n",
    "    try:\n",
    "        return TextBlob(str(clean_data)).sentiment.polarity\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "df['Polarity of Feedback'] = df['Clean_Text'].apply(sentiment_calc)\n",
    "df.head()\n",
    "df.to_csv('sentiment_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of Frequency Distribution and pos tag from nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 8, ',': 5, 'stemming': 3, 'lemmatization': 3, 'data': 3, 'language': 3, 'Tokenization': 2, 'text': 2, 'tokens': 2, 'elements': 2, ...})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "a = \"Tokenization is the process by which a large quantity of text is divided into smaller parts called tokens. These tokens are very useful for finding patterns and are considered as a base step for stemming and lemmatization. Tokenization also helps to substitute sensitive data elements with non-sensitive data elements. Natural language processing is used for building applications such as Text classification, intelligent chatbot, sentimental analysis, language translation, etc. It becomes vital to understand the pattern in the text to achieve the above-stated purpose. For the time being, don't worry about stemming and lemmatization but treat them as steps for textual data cleaning using NLP (Natural language processing). We will discuss stemming and lemmatization later in the tutorial. Tasks such as Text classification or spam filtering makes use of NLP along with deep learning libraries such as Keras and Tensorflow.\"\n",
    "token_word = word_tokenize(a)\n",
    "token_word = [w for w in token_word if not w in stop]\n",
    "words_analysis = FreqDist(token_word)\n",
    "words_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pos tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NN'),\n",
       " ('!', '.'),\n",
       " ('Tokenization', 'NN'),\n",
       " ('process', 'NN'),\n",
       " ('large', 'JJ'),\n",
       " ('quantity', 'NN'),\n",
       " ('text', 'NN'),\n",
       " ('divided', 'VBD'),\n",
       " ('smaller', 'JJR'),\n",
       " ('parts', 'NNS'),\n",
       " ('called', 'VBD'),\n",
       " ('tokens', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('These', 'DT'),\n",
       " ('tokens', 'NNS'),\n",
       " ('useful', 'JJ'),\n",
       " ('finding', 'VBG'),\n",
       " ('patterns', 'NNS'),\n",
       " ('considered', 'VBN'),\n",
       " ('base', 'JJ'),\n",
       " ('step', 'NN'),\n",
       " ('stemming', 'VBG'),\n",
       " ('lemmatization', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Tokenization', 'NNP'),\n",
       " ('also', 'RB'),\n",
       " ('helps', 'VBZ'),\n",
       " ('substitute', 'VB'),\n",
       " ('sensitive', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('elements', 'NNS'),\n",
       " ('non-sensitive', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('elements', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('used', 'VBN'),\n",
       " ('building', 'NN'),\n",
       " ('applications', 'NNS'),\n",
       " ('Text', 'NNP'),\n",
       " ('classification', 'NN'),\n",
       " (',', ','),\n",
       " ('intelligent', 'JJ'),\n",
       " ('chatbot', 'NN'),\n",
       " (',', ','),\n",
       " ('sentimental', 'JJ'),\n",
       " ('analysis', 'NN'),\n",
       " (',', ','),\n",
       " ('language', 'NN'),\n",
       " ('translation', 'NN'),\n",
       " (',', ','),\n",
       " ('etc', 'FW'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('becomes', 'VBZ'),\n",
       " ('vital', 'JJ'),\n",
       " ('understand', 'JJ'),\n",
       " ('pattern', 'NN'),\n",
       " ('text', 'NN'),\n",
       " ('achieve', 'VBP'),\n",
       " ('above-stated', 'JJ'),\n",
       " ('purpose', 'NN'),\n",
       " ('.', '.'),\n",
       " ('For', 'IN'),\n",
       " ('time', 'NN'),\n",
       " (',', ','),\n",
       " (\"n't\", 'RB'),\n",
       " ('worry', 'VB'),\n",
       " ('stemming', 'VBG'),\n",
       " ('lemmatization', 'NN'),\n",
       " ('treat', 'NN'),\n",
       " ('steps', 'NNS'),\n",
       " ('textual', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('cleaning', 'VBG'),\n",
       " ('using', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Natural', 'NNP'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " (')', ')'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('discuss', 'VBP'),\n",
       " ('stemming', 'VBG'),\n",
       " ('lemmatization', 'NN'),\n",
       " ('later', 'RB'),\n",
       " ('tutorial', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Tasks', 'NNP'),\n",
       " ('Text', 'NNP'),\n",
       " ('classification', 'NN'),\n",
       " ('spam', 'NN'),\n",
       " ('filtering', 'VBG'),\n",
       " ('makes', 'VBZ'),\n",
       " ('use', 'NN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('along', 'RB'),\n",
       " ('deep', 'RB'),\n",
       " ('learning', 'VBG'),\n",
       " ('libraries', 'NNS'),\n",
       " ('Keras', 'NNP'),\n",
       " ('Tensorflow', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = \" Hello! Tokenization is the process by which a large quantity of text is divided into smaller parts called tokens. These tokens are very useful for finding patterns and are considered as a base step for stemming and lemmatization. Tokenization also helps to substitute sensitive data elements with non-sensitive data elements. Natural language processing is used for building applications such as Text classification, intelligent chatbot, sentimental analysis, language translation, etc. It becomes vital to understand the pattern in the text to achieve the above-stated purpose. For the time being, don't worry about stemming and lemmatization but treat them as steps for textual data cleaning using NLP (Natural language processing). We will discuss stemming and lemmatization later in the tutorial. Tasks such as Text classification or spam filtering makes use of NLP along with deep learning libraries such as Keras and Tensorflow.\"\n",
    "words = word_tokenize(b)\n",
    "words = [w for w in words if not w in stop]\n",
    "tagged = nltk.pos_tag(words)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email and Phone number Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your EmailID: rinkal25@gmail.com\n",
      "Valid Email\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "email = input(\"Enter your EmailID: \")\n",
    "pattern1 = '^(\\w|\\.|\\_|\\-)+[@](\\w|\\_|\\-|\\.)+[.]\\w{2,3}$'\n",
    "\n",
    "matched_pattern = re.match(pattern1,email)\n",
    "\n",
    "if matched_pattern:\n",
    "    print(\"Valid Email\")\n",
    "else:\n",
    "    print(\"Invalid Email\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone number Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Phone number with Country Code: +915989898989\n",
      "Invalid Phone Number\n"
     ]
    }
   ],
   "source": [
    "phn_num = input(\"Enter your Phone number with Country Code: \")\n",
    "pattern = '^\\+91?[7-9][0-9]{9}$'\n",
    "\n",
    "check_num = re.match(pattern,phn_num)\n",
    "\n",
    "if check_num:\n",
    "    print(\"Valid Phone Number\")\n",
    "else:\n",
    "    print(\"Invalid Phone Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
